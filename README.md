# üß† Synthetic Personality Analysis with NLP

> Predicting hobbies and interests from synthetic personality descriptions using embedding-based NLP techniques.

**Authors:** Yasin Ye≈üilyurt & Abdullah Arda G√ºndoƒüdu  
**Course:** BIL 470 ‚Äî Senior Project  

---

## üìå Overview

This project explores whether **natural language descriptions of a person's personality** can predict their **hobbies and interests**. We build a full NLP pipeline that:

1. **Extracts & cleans** hobby data from a large synthetic persona dataset ([NVIDIA Nemotron Personas](https://huggingface.co/datasets/nvidia/Nemotron-Personas))
2. **Reduces hobby noise** via compression-based clustering (Normalized Compression Distance)
3. **Generates dense embeddings** using Qwen 3 language models (0.6B & 8B)
4. **Predicts hobbies** from persona embeddings using k-NN retrieval and Cross-Encoder re-ranking
5. **Learns a direct projection** from persona ‚Üí hobby embedding space via a deep neural network

---

## üèóÔ∏è Project Structure

```
.
‚îú‚îÄ‚îÄ üìÅ embeds/                          # Pre-computed persona embeddings
‚îÇ   ‚îú‚îÄ‚îÄ v0.1/                           # Qwen 3 0.6B persona embeddings
‚îÇ   ‚îú‚îÄ‚îÄ v0.1_8B/                        # Qwen 3 8B persona embeddings
‚îÇ   ‚îî‚îÄ‚îÄ embed_exploration.ipynb         # Embedding analysis notebook
‚îÇ
‚îú‚îÄ‚îÄ üìÅ initial_expedition/              # Early exploration & prototyping
‚îÇ   ‚îú‚îÄ‚îÄ dataset_expedition_hobbies.ipynb
‚îÇ   ‚îú‚îÄ‚îÄ dataset_fixed.ipynb
‚îÇ   ‚îú‚îÄ‚îÄ canonical_embeddings_*.npy      # V1 embedding outputs
‚îÇ   ‚îî‚îÄ‚îÄ semantically_merged_hobbies*.csv
‚îÇ
‚îú‚îÄ‚îÄ üìÅ expedition_v2/                   # V2 pipeline (production)
‚îÇ   ‚îú‚îÄ‚îÄ pipeline_runner.py              # End-to-end orchestrator
‚îÇ   ‚îú‚îÄ‚îÄ clustering_ncd.py               # NCD-based hobby clustering
‚îÇ   ‚îú‚îÄ‚îÄ embedding_factory.py            # Qwen embedding generator
‚îÇ   ‚îú‚îÄ‚îÄ evaluate_quality.py             # Cluster & embedding evaluation
‚îÇ   ‚îú‚îÄ‚îÄ output/                         # Generated clusters + embeddings
‚îÇ   ‚îî‚îÄ‚îÄ logs/                           # Pipeline execution logs
‚îÇ
‚îú‚îÄ‚îÄ üìÅ prediction/                      # Hobby prediction engines
‚îÇ   ‚îú‚îÄ‚îÄ base_predictor.py               # Abstract predictor interface
‚îÇ   ‚îú‚îÄ‚îÄ prediction_MatrixKNN.py         # Matrix k-NN predictor
‚îÇ   ‚îú‚îÄ‚îÄ prediction_CrossEncoder.py      # Two-stage Cross-Encoder predictor
‚îÇ   ‚îú‚îÄ‚îÄ similarity_utils.py             # Vector similarity & k-NN utilities
‚îÇ   ‚îú‚îÄ‚îÄ final_dataset.csv               # Final persona dataset
‚îÇ   ‚îú‚îÄ‚îÄ matrixKNN.ipynb                 # Interactive MatrixKNN exploration
‚îÇ   ‚îî‚îÄ‚îÄ cross_encoding.ipynb            # Cross-Encoder experiments
‚îÇ
‚îú‚îÄ‚îÄ üìÅ vector_to_vector_reggression/    # Deep learning projection
‚îÇ   ‚îú‚îÄ‚îÄ vector_projection_model.py      # HobbyProjector neural network
‚îÇ   ‚îú‚îÄ‚îÄ training_util.py                # Training loop & dataset class
‚îÇ   ‚îú‚îÄ‚îÄ y_projection.py                 # Persona ‚Üí hobby dataset builder
‚îÇ   ‚îú‚îÄ‚îÄ training_nemotron.ipynb         # Training notebook
‚îÇ   ‚îî‚îÄ‚îÄ projection_dataset.npz         # Cached (X, y) dataset
‚îÇ
‚îú‚îÄ‚îÄ üìÅ training_results/                # Saved model checkpoints (.pth)
‚îÇ
‚îú‚îÄ‚îÄ test_prediction.py                  # Integration tests for MatrixKNN
‚îú‚îÄ‚îÄ test_similarity.py                  # Unit tests for similarity utils
‚îú‚îÄ‚îÄ extract_pdf_text.py                 # PDF text extraction utility
‚îî‚îÄ‚îÄ .gitignore
```

---

## üî¨ Methodology

### 1. Data Preparation

The raw data comes from the [NVIDIA Nemotron-Personas](https://huggingface.co/datasets/nvidia/Nemotron-Personas) dataset ‚Äî a large-scale synthetic dataset of personality profiles. We extract hobby/interest fields and flatten them into a unique list of activity strings.

### 2. Hobby Clustering (NCD)

Raw hobbies are noisy and contain many near-duplicates (e.g., *"playing soccer"* vs. *"soccer"*). We use **Normalized Compression Distance (NCD)** ‚Äî a parameter-free string similarity metric based on Kolmogorov complexity ‚Äî combined with **Agglomerative Clustering** to merge semantically identical hobbies into canonical groups.

```
NCD(x, y) = [ C(xy) ‚àí min(C(x), C(y)) ] / max(C(x), C(y))
```

This dramatically reduces the hobby vocabulary while preserving semantic diversity.

### 3. Embedding Generation

We generate dense vector representations for both canonical hobbies and persona descriptions using **Qwen 3 Embedding** models:

| Model | Parameters | Use Case |
|-------|-----------|----------|
| `Qwen/Qwen3-Embedding-0.6B` | 0.6B | Lightweight experiments |
| `Qwen/Qwen3-Embedding-8B` | 8B | High-quality production embeddings |

Embeddings are produced via mean pooling over the last hidden state of tokenized inputs.

### 4. Hobby Prediction

We implement two prediction strategies, both following a common `BasePredictor` interface:

- **MatrixKNN** ‚Äî Vectorized exact k-nearest neighbor search using matrix multiplication for cosine/euclidean similarity. Fast and effective for embedding retrieval.
- **CrossEncoder (Two-Stage)** ‚Äî First retrieves candidates via MatrixKNN, then re-ranks them with a `cross-encoder/ms-marco-MiniLM-L-6-v2` model for improved precision.

### 5. Vector-to-Vector Projection (Deep Learning)

Instead of retrieval-based matching, we also train a **HobbyProjector** neural network to directly learn the mapping from persona embedding space ‚Üí hobby embedding space:

```
Persona Vector (4096-d)  ‚Üí  MLP(4096‚Üí2048‚Üí2048‚Üí4096)  ‚Üí  Hobby Vector (4096-d)
```

- Architecture: 3-layer MLP with ReLU + Dropout (0.2) + L2-normalized output
- Loss: `CosineEmbeddingLoss` (maximizes directional similarity)
- Optimizer: Adam (lr=1e-4)

---

## üöÄ Getting Started

### Prerequisites

- Python 3.9+
- CUDA-capable GPU (recommended for embedding generation and training)

### Installation

```bash
git clone https://github.com/cheeseburgerhere/Synthetic-Personality-Analysis-with-NLP.git
cd Synthetic-Personality-Analysis-with-NLP
pip install -r requirements.txt
```

> **Note:** If a `requirements.txt` is not yet available, install the following core dependencies:
> ```bash
> pip install numpy pandas scikit-learn torch transformers sentence-transformers matplotlib
> ```

### Running the Pipeline

**1. NCD Clustering + Embedding Generation:**
```bash
python expedition_v2/pipeline_runner.py --input_path all_hobbies.json --limit 0
```
- `--limit 0` processes all hobbies; set a smaller value for testing.

**2. Build Projection Dataset:**
```bash
python vector_to_vector_reggression/y_projection.py
```

**3. Train the Projector Model:**
```bash
python vector_to_vector_reggression/training_util.py
```

**4. Evaluate Clustering Quality:**
```bash
python expedition_v2/evaluate_quality.py
```

### Running Tests

```bash
python test_similarity.py
python test_prediction.py
```

---

## üìä Key Results

- **Hobby Clustering** significantly reduces vocabulary size while preserving semantic coverage.
- **Matrix k-NN** provides fast, reliable baseline predictions using cosine similarity.
- **Cross-Encoder re-ranking** improves prediction nuance by jointly encoding persona-hobby pairs.
- **HobbyProjector** learns a direct, generalizable mapping between embedding spaces.

---

## üõ†Ô∏è Tech Stack

| Category | Technologies |
|----------|-------------|
| **Language** | Python 3.9+ |
| **Deep Learning** | PyTorch |
| **NLP Models** | Qwen 3 Embedding (0.6B, 8B), MiniLM Cross-Encoder |
| **ML Utilities** | scikit-learn, NumPy, Pandas |
| **Transformers** | Hugging Face `transformers`, `sentence-transformers` |
| **Visualization** | Matplotlib, t-SNE |

---

## üìÑ License

This project was developed as part of the BIL 470 Senior Project course. Please contact the authors for usage permissions.
