{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ba02dc15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from huggingface_hub import login\n",
    "from datasets import load_dataset\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bcaa4f39",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.\n"
     ]
    }
   ],
   "source": [
    "login(token = os.getenv(\"HF_TOKEN\"))\n",
    "dataset = load_dataset(\"nvidia/Nemotron-Personas\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c309d21d",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../Nemotron_Personas.parquet'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_parquet\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m../Nemotron_Personas.parquet\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39miloc[:\u001b[38;5;241m3000\u001b[39m,:]\n",
      "File \u001b[0;32m~/miniconda3/envs/spa/lib/python3.10/site-packages/pandas/io/parquet.py:669\u001b[0m, in \u001b[0;36mread_parquet\u001b[0;34m(path, engine, columns, storage_options, use_nullable_dtypes, dtype_backend, filesystem, filters, **kwargs)\u001b[0m\n\u001b[1;32m    666\u001b[0m     use_nullable_dtypes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    667\u001b[0m check_dtype_backend(dtype_backend)\n\u001b[0;32m--> 669\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mimpl\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    670\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    671\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    672\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfilters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    673\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    674\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_nullable_dtypes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_nullable_dtypes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    675\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype_backend\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype_backend\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    676\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfilesystem\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilesystem\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    677\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    678\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/spa/lib/python3.10/site-packages/pandas/io/parquet.py:258\u001b[0m, in \u001b[0;36mPyArrowImpl.read\u001b[0;34m(self, path, columns, filters, use_nullable_dtypes, dtype_backend, storage_options, filesystem, **kwargs)\u001b[0m\n\u001b[1;32m    256\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m manager \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marray\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    257\u001b[0m     to_pandas_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msplit_blocks\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 258\u001b[0m path_or_handle, handles, filesystem \u001b[38;5;241m=\u001b[39m \u001b[43m_get_path_or_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    259\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    260\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfilesystem\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    261\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    262\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    263\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    265\u001b[0m     pa_table \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapi\u001b[38;5;241m.\u001b[39mparquet\u001b[38;5;241m.\u001b[39mread_table(\n\u001b[1;32m    266\u001b[0m         path_or_handle,\n\u001b[1;32m    267\u001b[0m         columns\u001b[38;5;241m=\u001b[39mcolumns,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    270\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    271\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/spa/lib/python3.10/site-packages/pandas/io/parquet.py:141\u001b[0m, in \u001b[0;36m_get_path_or_handle\u001b[0;34m(path, fs, storage_options, mode, is_dir)\u001b[0m\n\u001b[1;32m    131\u001b[0m handles \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    132\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    133\u001b[0m     \u001b[38;5;129;01mnot\u001b[39;00m fs\n\u001b[1;32m    134\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_dir\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    139\u001b[0m     \u001b[38;5;66;03m# fsspec resources can also point to directories\u001b[39;00m\n\u001b[1;32m    140\u001b[0m     \u001b[38;5;66;03m# this branch is used for example when reading from non-fsspec URLs\u001b[39;00m\n\u001b[0;32m--> 141\u001b[0m     handles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    142\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpath_or_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\n\u001b[1;32m    143\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    144\u001b[0m     fs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    145\u001b[0m     path_or_handle \u001b[38;5;241m=\u001b[39m handles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[0;32m~/miniconda3/envs/spa/lib/python3.10/site-packages/pandas/io/common.py:882\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\n\u001b[1;32m    874\u001b[0m             handle,\n\u001b[1;32m    875\u001b[0m             ioargs\u001b[38;5;241m.\u001b[39mmode,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    878\u001b[0m             newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    879\u001b[0m         )\n\u001b[1;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m--> 882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    883\u001b[0m     handles\u001b[38;5;241m.\u001b[39mappend(handle)\n\u001b[1;32m    885\u001b[0m \u001b[38;5;66;03m# Convert BytesIO or file objects passed with an encoding\u001b[39;00m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../Nemotron_Personas.parquet'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_parquet(\"../Nemotron_Personas.parquet\").iloc[:3000,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f6ebd41",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76bd9ac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "046e504e",
   "metadata": {},
   "outputs": [],
   "source": [
    "not_required_columns=['zipcode', 'country']\n",
    "persona_columns=['persona', 'professional_persona', 'sports_persona',\n",
    "                'arts_persona', 'travel_persona', 'culinary_persona']\n",
    "categorical_columns=['sex', 'marital_status','education_level', \n",
    "                     'bachelors_field', 'occupation', 'city', 'state']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "783ab115",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[persona_columns].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10f0174f",
   "metadata": {},
   "source": [
    "## expertise_list\n",
    "\n",
    "-TF-IDF "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "770583d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "df['hobbies_and_interests_list'] = df['hobbies_and_interests_list'].apply(ast.literal_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "846f4c84",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_hobbies = set()\n",
    "for hobbies in df['hobbies_and_interests_list']:\n",
    "    total_hobbies.update(hobbies)\n",
    "\n",
    "\n",
    "len(total_hobbies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60950484",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_hobbies\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f210221",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "JSON_FILE = '../all_hobbies.json'\n",
    "\n",
    "try:\n",
    "    with open(JSON_FILE, 'r') as f:\n",
    "        total_hobbies = json.load(f)\n",
    "    print(\"Successfully loaded 'total_hobbies' from JSON file.\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"JSON file '{JSON_FILE}' not found.\")\n",
    "    with open(JSON_FILE, 'w') as f:\n",
    "        json.dump(list(total_hobbies), f)\n",
    "\n",
    "except json.JSONDecodeError:\n",
    "    print(f\"Error decoding JSON from file '{JSON_FILE}'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0510eedf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import networkx as nx\n",
    "\n",
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "hobby_list = list(total_hobbies)\n",
    "embeddings = model.encode(hobby_list, show_progress_bar=True)\n",
    "\n",
    "similarity_matrix = cosine_similarity(embeddings)\n",
    "\n",
    "threshold = 0.75\n",
    "\n",
    "\n",
    "\n",
    "G = nx.Graph()\n",
    "G.add_nodes_from(range(len(hobby_list)))\n",
    "\n",
    "for i in range(len(hobby_list)):\n",
    "    for j in range(i + 1, len(hobby_list)):\n",
    "        if similarity_matrix[i, j] > threshold:\n",
    "            G.add_edge(i, j)\n",
    "\n",
    "merged_hobbies = []\n",
    "for component in nx.connected_components(G):\n",
    "    group = [hobby_list[idx] for idx in component]\n",
    "    merged_hobbies.append(group)\n",
    "\n",
    "canonical_hobbies = [g[0] for g in merged_hobbies]\n",
    "\n",
    "df_merged = pd.DataFrame({\n",
    "    \"canonical_hobby\": canonical_hobbies,\n",
    "    \"merged_group\": [\", \".join(g) for g in merged_hobbies]\n",
    "})\n",
    "\n",
    "df_merged.to_csv(\"semantically_merged_hobbies.csv\", index=False)\n",
    "print(f\"✅ {len(canonical_hobbies)} unique semantic hobby groups created and saved to semantically_merged_hobbies.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0ac379f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "model_name = \"Qwen/Qwen3-Embedding-0.6B\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name)\n",
    "\n",
    "\n",
    "df_merged = pd.read_csv(\"semantically_merged_hobbies.csv\")\n",
    "hobbies = df_merged[\"canonical_hobby\"].tolist()\n",
    "\n",
    "\n",
    "def get_embedding(text):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=128)\n",
    "    with torch.no_grad():\n",
    "        output = model(**inputs)\n",
    "    return output.last_hidden_state.mean(dim=1).squeeze().numpy()\n",
    "\n",
    "\n",
    "embeddings = []\n",
    "for hobby in tqdm(hobbies, desc=\"Creating embeddings\"):\n",
    "    embeddings.append(get_embedding(hobby))\n",
    "\n",
    "embeddings = np.array(embeddings)\n",
    "np.save(\"canonical_embeddings_qwen.npy\", embeddings)\n",
    "\n",
    "print(f\"✅ Embeddings created for {len(hobbies)} hobbies and saved as canonical_embeddings_qwen.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hdbscan_install",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install hdbscan scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hdbscan_run",
   "metadata": {},
   "outputs": [],
   "source": [
    "import hdbscan\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Load embeddings if not in memory (optional, assuming they are from previous cell)\n",
    "# embeddings = np.load(\"canonical_embeddings_qwen.npy\")\n",
    "\n",
    "clusterer = hdbscan.HDBSCAN(min_cluster_size=3, min_samples=1, metric='euclidean')\n",
    "cluster_labels = clusterer.fit_predict(embeddings)\n",
    "\n",
    "df_hdbscan = pd.DataFrame({\n",
    "    \"hobby\": hobbies,\n",
    "    \"cluster\": cluster_labels\n",
    "})\n",
    "\n",
    "num_clusters = len(set(cluster_labels)) - (1 if -1 in cluster_labels else 0)\n",
    "num_noise = list(cluster_labels).count(-1)\n",
    "\n",
    "print(f\"HDBSCAN found {num_clusters} clusters and {num_noise} noise points.\")\n",
    "# df_hdbscan.to_csv(\"hdbscan_clusters.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "install_eval_deps",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install umap-learn matplotlib seaborn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "calc_metrics",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import silhouette_score, davies_bouldin_score\n",
    "\n",
    "# Filter out noise points (-1) for metric calculation if desired, \n",
    "# but often we want to know how well the clustered points are separated.\n",
    "# Here we calculate metrics on clustered data only.\n",
    "clustered_mask = cluster_labels != -1\n",
    "if clustered_mask.sum() > 1:\n",
    "    sil_score = silhouette_score(embeddings[clustered_mask], cluster_labels[clustered_mask])\n",
    "    db_score = davies_bouldin_score(embeddings[clustered_mask], cluster_labels[clustered_mask])\n",
    "    print(f\"Silhouette Score: {sil_score:.3f} (closer to 1 is better)\")\n",
    "    print(f\"Davies-Bouldin Index: {db_score:.3f} (lower is better)\")\n",
    "else:\n",
    "    print(\"Not enough clustered points to calculate metrics.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "umap_viz",
   "metadata": {},
   "outputs": [],
   "source": [
    "import umap\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "reducer = umap.UMAP(n_neighbors=15, min_dist=0.1, metric='cosine', random_state=42)\n",
    "embedding_2d = reducer.fit_transform(embeddings)\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "# Plot noise in grey\n",
    "noise_mask = cluster_labels == -1\n",
    "plt.scatter(embedding_2d[noise_mask, 0], embedding_2d[noise_mask, 1], c='grey', s=10, alpha=0.3, label='Noise')\n",
    "\n",
    "# Plot clusters\n",
    "clustered_mask = ~noise_mask\n",
    "plt.scatter(embedding_2d[clustered_mask, 0], embedding_2d[clustered_mask, 1], \n",
    "            c=cluster_labels[clustered_mask], cmap='Spectral', s=20, alpha=0.8)\n",
    "\n",
    "plt.title('UMAP Projection of Hobby Clusters', fontsize=16)\n",
    "plt.xlabel('UMAP 1')\n",
    "plt.ylabel('UMAP 2')\n",
    "plt.colorbar(label='Cluster Label')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sample_inspection",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Random samples from top clusters:\")\n",
    "top_clusters = df_hdbscan['cluster'].value_counts().head(5).index.tolist()\n",
    "if -1 in top_clusters:\n",
    "    top_clusters.remove(-1)\n",
    "\n",
    "for cluster_id in top_clusters:\n",
    "    print(f\"\\nCluster {cluster_id}:\")\n",
    "    sample = df_hdbscan[df_hdbscan['cluster'] == cluster_id]['hobby'].sample(min(5, len(df_hdbscan[df_hdbscan['cluster'] == cluster_id])))\n",
    "    for item in sample:\n",
    "        print(f\" - {item}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "prop_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
